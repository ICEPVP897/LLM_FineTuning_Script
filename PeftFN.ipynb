{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r0N9hbYvS13_"
      },
      "outputs": [],
      "source": [
        "## Install required libraries\n",
        "!pip install -q transformers datasets accelerate bitsandbytes peft trl torch\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q llama-cpp-python\n",
        "!pip install -q ctranslate2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhdEL2mkS2EV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset, concatenate_datasets\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from trl import SFTTrainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFX1u4KdS2Mq"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"unsloth/gemma-2-9b\"  # Replace with the actual model name  {SicariusSicariiStuff/Phi-3.5-mini-instruct_Uncensored}\n",
        "\"\"\"Models to be trained:\n",
        "_________________________________\n",
        "|                                |\n",
        "|  1.google/gemma-2-9b           |\n",
        "|  2.google/gemma-2-2b           |\n",
        "|  3.google/gemma-2-2b-it        |\n",
        "|  4.google/codegemma-2b         |\n",
        "|  5.google/codegemma-7b-it      |\n",
        "|                                |\n",
        "|________________________________|\n",
        "\"\"\"\n",
        "\n",
        "# use_auth_token=\"hf-token_from_huggingface\" \n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME,\n",
        "                                        #   use_auth_token=\"\"\n",
        "                                        )\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "## Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    # use_auth_token=\"\"\n",
        ")\n",
        "\n",
        "## Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "## LoRA configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "## Apply LoRA to the model\n",
        "model = get_peft_model(model, peft_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhf0hEPqQc7B"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples.get(\"instruction\", examples.get(\"instruction\",examples.get(\"system\", [])))\n",
        "    inputs = examples.get(\"input\", examples.get(\"input\",examples.get(\"command\", [])))\n",
        "    outputs = examples.get(\"output\", examples.get(\"Output\", examples.get(\"response\", examples.get(\"Response\", []))))\n",
        "    # return instructions, inputs, outputs  # Remove this line causing the error\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts} # Return a dictionary as expected by map function\n",
        "\n",
        "def standardize_columns(dataset):\n",
        "    rename_dict = {\n",
        "        \"Response\": \"output\",\n",
        "        \"response\": \"output\",\n",
        "        \"Output\": \"output\",\n",
        "        \"Input\": \"input\",\n",
        "        \"Instruction\": \"instruction\",\n",
        "        \"Instruction\": \"system\"\n",
        "    }\n",
        "    return dataset.rename_columns({k: v for k, v in rename_dict.items() if k in dataset.column_names})\n",
        "\n",
        "\n",
        "# Insert the Dataset repo_id below to load the dataset from the repo: \n",
        "\n",
        "datasets_to_load = [\n",
        "  \"ICEPVP8977/Debian_Hacking_Networking\",\n",
        "  \"ICEPVP8977/Uncensored_mini\",\n",
        "]\n",
        "\n",
        "def has_train_split(dataset_name):\n",
        "    try:\n",
        "        dataset_info = load_dataset(dataset_name, split=None)\n",
        "        return 'train' in dataset_info.keys()\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "datasets_with_train_split = [dataset_name for dataset_name in datasets_to_load if has_train_split(dataset_name)]\n",
        "\n",
        "datasets = []\n",
        "for dataset_name in datasets_with_train_split:\n",
        "    try:\n",
        "        dataset = load_dataset(dataset_name, split=\"train\")\n",
        "        standardized_dataset = standardize_columns(dataset)\n",
        "\n",
        "        required_columns = [\"instruction\", \"input\", \"output\"]\n",
        "        if all(col in standardized_dataset.column_names for col in required_columns):\n",
        "            datasets.append(standardized_dataset)\n",
        "            print(f\"Successfully loaded and standardized: {dataset_name}\")\n",
        "        else:\n",
        "            print(f\"Skipping {dataset_name}: Missing required columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
        "\n",
        "combined_dataset = concatenate_datasets(datasets)\n",
        "\n",
        "formatted_dataset = combined_dataset.map(formatting_prompts_func, batched=True, remove_columns=combined_dataset.column_names)\n",
        "\n",
        "formatted_dataset = formatted_dataset.shuffle(seed=199)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEW7J0cUTDXP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define training arguments without max_seq_length\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,#Default = 3 - 4\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=2e-4,# For general and fast adaptation {5e-5} is generally recommended./-- For the model to reproduce the exact text from the datasets the learning rate {1e-5} or even lower.\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    save_total_limit=3,\n",
        "    push_to_hub=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFX_YJ6xZK0k"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Use a data collator for dynamic padding\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Set to False for causal language modeling\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv0lqBEgZLC8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,  # Use the data collator\n",
        "    packing=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3lxm7EZTG6l"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYTHBABfUTFr"
      },
      "outputs": [],
      "source": [
        "## Save the final model\n",
        "trainer.model.save_pretrained(\"./final_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFwfqVoEkfwu"
      },
      "outputs": [],
      "source": [
        "model = trainer.model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8Gdpd-Bkg4A"
      },
      "outputs": [],
      "source": [
        "# Save the full model to 4bit\n",
        "model.save_pretrained(\"./full_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUzhpMaPkjLG"
      },
      "outputs": [],
      "source": [
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(\"./full_model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
